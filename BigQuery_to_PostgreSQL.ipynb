{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135aaa9c-9943-4d9a-aa29-a53a519ec83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "from google.oauth2 import service_account\n",
    "from io import BytesIO\n",
    "import psycopg2\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "from sqlalchemy import create_engine, inspect, MetaData, text\n",
    "from google.cloud.exceptions import GoogleCloudError\n",
    "from google.api_core.exceptions import GoogleAPIError, NotFound, Forbidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd41abb0-5660-4342-9358-d3ef7252e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentialsPath = \"C:/Users/AU TRADERS/Downloads/noman-gcu-iot-jan24-0a4c8b2033d6.json\"\n",
    "bq_credentials = service_account.Credentials.from_service_account_file(credentialsPath)\n",
    "gcs_credentials = service_account.Credentials.from_service_account_file(credentialsPath)\n",
    "project_id = 'noman-gcu-iot-jan24'\n",
    "destination_uri = 'gs://partitioned_table'\n",
    "bucket_name = 'partitioned_table'\n",
    "dataset_id = 'BikeStore'\n",
    "full_dataset_id = 'noman-gcu-iot-jan24.BikeStore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81058d6e-da28-49f1-bcc6-30837b4e5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = \"test123\"\n",
    "uid = \"python\"\n",
    "pg_database = 'Partitioned_Tables'\n",
    "\n",
    "postgres_uri = f\"postgresql+psycopg2://{uid}:{pwd}@localhost:5432/{pg_database}\"\n",
    "postgres_engine = create_engine(postgres_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3533dc5f-015f-4db5-a134-6a29fec578ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_clients(bq_creds, gcs_creds, proj_id):\n",
    "    try:\n",
    "        storage_client = storage.Client(credentials=gcs_creds, project=proj_id)\n",
    "        bigquery_client = bigquery.Client(credentials=bq_creds, project=proj_id)\n",
    "        return storage_client, bigquery_client\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Failed to initialize clients due to a Google API error: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while initializing clients: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a510b364-f937-46b8-8c3a-19236b014856",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_client, bq_client = initialize_clients(bq_credentials, gcs_credentials, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2569dc24-50f4-4887-bfdc-bd84396e96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yesterdays_date():\n",
    "    yesterday = datetime.now() - timedelta(days=1)\n",
    "    return yesterday.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234455fe-1ca4-401c-92c8-6c3f2a7fcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_partitioned_tables(bq_client, dataset_id):\n",
    "    try:\n",
    "        dataset_ref = bq_client.dataset(dataset_id)\n",
    "        tables = bq_client.list_tables(dataset_ref)\n",
    "        partitioned_tables = []\n",
    "\n",
    "        for table in tables:\n",
    "            try:\n",
    "                table_ref = bq_client.get_table(table.reference)\n",
    "                if table_ref.time_partitioning or table_ref.range_partitioning:\n",
    "                    partitioned_tables.append(table.table_id)\n",
    "            except NotFound:\n",
    "                logging.error(f\"Table {table.table_id} not found.\")\n",
    "            except Forbidden:\n",
    "                logging.error(f\"Access to table {table.table_id} is forbidden.\")\n",
    "\n",
    "        return partitioned_tables\n",
    "    except NotFound:\n",
    "        logging.error(f\"Dataset {dataset_id} not found.\")\n",
    "        return []\n",
    "    except Forbidden:\n",
    "        logging.error(f\"Access to dataset {dataset_id} is forbidden.\")\n",
    "        return []\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bce9e150-0f4a-41e1-9306-5b1fc25b8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_yesterdays_partition(bq_client, table, full_dataset_id, yesterday):\n",
    "    new_partition = yesterday.replace('-', '')\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT MAX(partition_id) as max_partition_id\n",
    "    FROM `{full_dataset_id}.INFORMATION_SCHEMA.PARTITIONS`\n",
    "    WHERE table_name = '{table}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = bq_client.query(query)\n",
    "        result = query_job.result()\n",
    "        partition_id = next(result, None).max_partition_id if result.total_rows > 0 else None\n",
    "        return partition_id if partition_id == new_partition else None\n",
    "    except bigquery.exceptions.BigQueryError as e:\n",
    "        logging.error(f\"BigQuery error processing table {table}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error processing table {table}: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07194cda-d3ad-4547-bcdb-e9c660aeaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitioning_field(client, table, dataset_id):\n",
    "    table_id = f'{dataset_id}.{table}'\n",
    "    try:\n",
    "        bq_table = client.get_table(table_id)\n",
    "        return bq_table.time_partitioning.field if bq_table.time_partitioning else None\n",
    "    except bigquery.NotFound as e:\n",
    "        logging.error(f\"Table {table_id} not found: {e}\")\n",
    "        return None\n",
    "    except bigquery.Forbidden as e:\n",
    "        logging.error(f\"Access to table {table_id} is forbidden: {e}\")\n",
    "        return None\n",
    "    except bigquery.GoogleCloudError as e:\n",
    "        logging.error(f\"Google Cloud error accessing table {table_id}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error getting partitioning field for table {table_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5dde0de-0318-4ca7-8271-f2daf146cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_partition_to_csv(bucket_name, s_client, client, dataset_id, table, partition_field, partition_id, yesterday, destination_uri, location=\"us-west1\"):\n",
    "\n",
    "    destination_uri = f'{destination_uri}/{table}_{partition_id}.csv'\n",
    "    blob_name = f'{table}_{partition_id}.csv'\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{dataset_id}.{table}`\n",
    "    WHERE `{partition_field}` = '{yesterday}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        destination_blob = s_client.bucket(bucket_name).blob(blob_name)\n",
    "        destination_blob.content_type = 'text/csv'\n",
    "        query_job.result().to_dataframe().to_csv(destination_blob.open('w'), index=False)\n",
    "        logging.info(f\"Export successful to {destination_uri}\")\n",
    "    except GoogleCloudError as e:\n",
    "        logging.error(f\"Error exporting table due to a Google Cloud error: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"General error exporting table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c586767-dcdd-41c6-9fce-2ebf68cbf04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partitioned_table_20240428.csv',\n",
       " 'partitioned_table1_20240428.csv',\n",
       " 'partitioned_table2_20240428.csv',\n",
       " 'partitioned_table3_20240428.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_tables = list_partitioned_tables(bq_client, dataset_id)\n",
    "yesterday = get_yesterdays_date()\n",
    "gcs_files = []\n",
    "\n",
    "for table in partitioned_tables:\n",
    "    y_partitioned_id = check_for_yesterdays_partition(bq_client, table, full_dataset_id, yesterday)\n",
    "    if y_partitioned_id is not None:\n",
    "        file_name = f\"{table}_{y_partitioned_id}.csv\"\n",
    "        gcs_files.append(file_name)\n",
    "        partitioning_field = get_partitioning_field(bq_client, table, dataset_id)\n",
    "        (bucket_name, b_client, bq_client, dataset_id, table, partitioning_field, y_partitioned_id, yesterday, destination_uri)\n",
    "\n",
    "gcs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8eff0c2-1bcd-4cc3-9043-9227bbdb1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Dumping table No. 1 partitioned_table...\n",
      "   .. Wrote partitioned_table to PostgreSQL database\n",
      "\n",
      "##### Dumping table No. 2 partitioned_table1...\n",
      "   .. Wrote partitioned_table1 to PostgreSQL database\n",
      "\n",
      "##### Dumping table No. 3 partitioned_table2...\n",
      "   .. Wrote partitioned_table2 to PostgreSQL database\n",
      "\n",
      "##### Dumping table No. 4 partitioned_table3...\n",
      "   .. Wrote partitioned_table3 to PostgreSQL database\n"
     ]
    }
   ],
   "source": [
    "table_no = 0\n",
    "for file_name in gcs_files:\n",
    "\n",
    "    bucket = b_client.get_bucket(bucket_name)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    blob.download_to_file(buffer)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    df = pd.read_csv(buffer)\n",
    "\n",
    "    table, _ = os.path.splitext(file_name)\n",
    "    table_name = table.split('_')\n",
    "    table_name = \"_\".join(table_name[:2])\n",
    "    \n",
    "    \n",
    "    ################################################################\n",
    "    print()\n",
    "    print(f\"##### Dumping table No. {table_no + 1} {table_name}...\")\n",
    "    ################################################################\n",
    "    table_no = table_no + 1\n",
    "    \n",
    "    try:\n",
    "        postgres_connection = postgres_engine.connect()\n",
    "        \n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        if table_name == 'partitioned_table3':\n",
    "            df.to_sql(name=table_name, con=postgres_connection, schema='public',\n",
    "                  chunksize=5000, index=False, if_exists='replace')\n",
    "        else:\n",
    "            df.to_sql(name=table_name, con=postgres_connection, schema='public',\n",
    "                      chunksize=5000, index=False, if_exists='append')\n",
    "        \n",
    "        ################################################################\n",
    "        print(f\"   .. Wrote {table_name} to PostgreSQL database\")\n",
    "        ################################################################\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "    finally:\n",
    "        postgres_connection.close()\n",
    "postgres_engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0962d-723c-488c-856c-42b6b4226b99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
